---
layout:     post 
title:      "DNN的实现思路"
date:       2021-08-29 00:22:00 +0800 
categories: DeepLearning
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>


最近打算实现一个DNN计算的实现，并尝试对实现做优化加速计算。

基本思路是：

- 首先使用python实现一个最简单的版本，并对数据结构做优化。
- 然后按照python的实现写一个C++的版本。
- 最后对C++版本的代码进行优化。

由于目前缺少nv显卡，amd显卡的Rocm又不支持windows平台，因此暂时只优化CPU版本，优化CPU版本的主要方法是使用模板多态+AVX进行优化。

## DNN (Deep Neural Networks, 深度神经网络)

本文主要讲解DNN计算中主要涉及的公式计算和代码接口定义，具体实现和优化会在之后的文章中讲解。

### 符号

- 向量和矩阵使用大写字母表示
- 标量使用小写字母表示
- 函数(如激活函数等)使用希腊小写字母表示

```
说明：暂时还未增加LaTex插件，因此数学公式可能会比较丑，之后增加插件后会做修正
```

### 正向计算公式

下图是一个简单的只有3层的DNN，分别是输入层`X=(x1,x2,x3,x4,x5)`，隐藏层`H=(h1,h2,h3,h4)`，输出层`O=(o1,o2)`：

![DNN](/Images/DNN_1.png)

对于上述DNN网络，分别有`H=σ(WX+B)`和`O=σ(WH+B)`，即对于DNN的输入层外的任意一层有：`O=σ(WX+B)`，其中`X`为本层的输入向量,`W`为本层的权
重系数矩阵，`B`为本层的偏置向量，`σ`为本层的激活函数,`O`为本层的输出向量，且上层的输出为下层的输入。

因此在正向计算过程中，我们将主要实现上述公式。

### 反向计算公式

Loss暂时只实现均方误差，下面只讲均方误差的计算公式，其他Loss函数之后会陆续补充。

##### 均方误差(mean squared error)

对于任意一层有：E=1/2(O-Y)^2，其中O为本层输出，Y为期望输出

求E对W、B、X的导数：

- `E'(W)=(O-Y)⊙σ'⊙X`
- `E'(B)=(O-Y)⊙σ'`

令`δ=(O-Y)⊙σ'`可得第l层的loss导数为：`E'(Wl)=δl⊙X`，`E'(Bl)=δl`，推导可得`δl=W*diag(σ')*δl+1`(详细的推导过程可以参考其他博客，这里不再赘述)。

因此在反向计算过程中，我们将主要实现`E'(Wl)=δl⊙X`，`E'(Bl)=δl`，`δl=W*diag(σ')*δl+1`这三个公式。
